# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Test script to verify Orchestrator Kernel implementation."""

import asyncio
import logging
import yaml
from datetime import datetime

# Orchestrator kernel components
from orchestrator_kernel import (
    OrchestratorKernel, KernelConfig, ExecutionContext,
    GraphNode, NodeType, ExecutionGraph, GraphCompiler, GraphScheduler, GraphRegistry,
    ReactLoop, LoopConfig, AMLPolicyHook, OPAPolicyHook, ToolCall, ReactState,
    Request, RequestRouter, RouteConfig, WorkerNode, RequestPriority,
    LoadBalancer, WeightedRoundRobinLoadBalancer
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_execution_graph():
    """Test execution graph functionality."""
    print("\nðŸ§ª Testing Execution Graph")
    print("-" * 40)
    
    # Test graph compiler with YAML
    yaml_graph = """
name: test_workflow
version: 1.0.0

nodes:
  - id: start
    name: Start Node
    type: llm_call
    config:
      prompt: Initialize
    checkpoint: true
    
  - id: process
    name: Process Node
    type: tool_invocation
    config:
      tool: process_data
      
  - id: finish
    name: Finish Node
    type: finish

edges:
  - from: start
    to: process
  - from: process
    to: finish

entry_node: start
"""
    
    compiler = GraphCompiler()
    graph = compiler.compile_yaml(yaml_graph)
    
    assert graph.name == "test_workflow", "Graph name mismatch"
    assert len(graph.nodes) == 3, f"Expected 3 nodes, got {len(graph.nodes)}"
    assert len(graph.edges) == 2, f"Expected 2 edges, got {len(graph.edges)}"
    assert graph.validate(), "Graph validation failed"
    print("âœ… Graph compilation and validation passed")
    
    # Test graph registry
    registry = GraphRegistry()
    version = registry.register_graph(graph)
    
    assert version == "v1.0.0", f"Expected v1.0.0, got {version}"
    retrieved_graph = registry.get_graph("test_workflow")
    assert retrieved_graph is not None, "Failed to retrieve graph"
    print(f"âœ… Graph registry: registered as version {version}")
    
    # Test graph scheduler
    scheduler = GraphScheduler()
    
    # Register mock executors
    async def mock_llm_executor(node, inputs, context):
        return {"result": f"Executed {node.name}"}
    
    async def mock_tool_executor(node, inputs, context):
        return {"result": f"Tool {node.config.get('tool')} executed"}
    
    scheduler.register_executor(NodeType.LLM_CALL, mock_llm_executor)
    scheduler.register_executor(NodeType.TOOL_INVOCATION, mock_tool_executor)
    
    async def mock_finish_executor(node, inputs, context):
        return {"finished": True}
    
    scheduler.register_executor(NodeType.FINISH, mock_finish_executor)
    
    # Execute graph
    result = await scheduler.execute_graph(graph, {"initial": "context"})
    
    assert result["status"] == "completed", f"Expected completed, got {result['status']}"
    assert len(result["node_outputs"]) == 3, "Not all nodes executed"
    print("âœ… Graph execution completed successfully")
    
    return graph, registry, scheduler


async def test_react_loop():
    """Test ReAct loop with policy hooks."""
    print("\nðŸ§ª Testing ReAct Loop")
    print("-" * 40)
    
    # Create loop configuration with policy hooks
    aml_hook = AMLPolicyHook(
        aml_level=3,
        caps={
            "create_refund": {"max_amount": 100},
            "send_email": {"max_per_hour": 5}
        }
    )
    
    opa_hook = OPAPolicyHook()
    
    loop_config = LoopConfig(
        max_iterations=5,
        policy_hooks=[aml_hook, opa_hook]
    )
    
    react_loop = ReactLoop(loop_config)
    
    # Register mock tools
    async def mock_refund_tool(amount, reason, context):
        return {"status": "success", "refund_id": "REF-123"}
    
    react_loop.register_tool("create_refund", mock_refund_tool)
    
    # Set mock LLM handler
    async def mock_llm_handler(prompt, context):
        # Simulate different responses based on iteration
        if "iteration 1" in prompt:
            return {
                "thought": "Customer needs refund",
                "reasoning": "Valid reason provided",
                "action": "create_refund",
                "parameters": {"amount": 75, "reason": "damaged"},
                "confidence": 0.9
            }
        else:
            return {
                "thought": "Task complete",
                "action": "FINISH",
                "reasoning": "Refund processed"
            }
    
    react_loop.set_llm_handler(mock_llm_handler)
    
    # Test policy enforcement
    print("Testing policy hooks...")
    
    # Test allowed action
    tool_call = ToolCall(tool_name="create_refund", parameters={"amount": 75})
    state = ReactState()
    
    allowed, reason = await aml_hook.check(tool_call, state, {})
    assert allowed, f"Policy should allow $75 refund at AML 3"
    print("âœ… AML policy: $75 refund allowed")
    
    # Test denied action
    tool_call.parameters["amount"] = 150
    allowed, reason = await aml_hook.check(tool_call, state, {})
    assert not allowed, "Policy should deny $150 refund at AML 3"
    print(f"âœ… AML policy: $150 refund denied - {reason}")
    
    # Test OPA policy
    for i in range(3):
        tc = ToolCall(tool_name="external_api", parameters={})
        state.tool_calls.append(tc)
    
    tool_call = ToolCall(tool_name="external_api", parameters={})
    allowed, reason = await opa_hook.check(tool_call, state, {})
    assert not allowed, "OPA should deny 4th external API call"
    print(f"âœ… OPA policy: 4th API call denied - {reason}")
    
    return react_loop, loop_config


async def test_request_routing():
    """Test request routing and load balancing."""
    print("\nðŸ§ª Testing Request Routing")
    print("-" * 40)
    
    # Create router configuration
    route_config = RouteConfig(
        load_balancing_strategy="weighted_round_robin",
        enable_circuit_breaker=True,
        circuit_breaker_threshold=0.5,
        enable_sticky_sessions=True
    )
    
    router = RequestRouter(route_config)
    
    # Register test workers
    workers = []
    for i in range(3):
        worker = WorkerNode(
            name=f"test-worker-{i+1}",
            host=f"worker-{i+1}.local",
            port=8080,
            pillar="test_pillar",
            agent_types={"test_agent"},
            capacity=5,
            cpu_usage=20 + i * 10,
            memory_usage=30 + i * 5,
            latency_ms=50 + i * 10,
            error_rate=0.05 * i
        )
        router.register_worker(worker)
        workers.append(worker)
    
    print(f"âœ… Registered {len(workers)} workers")
    
    # Test request routing
    requests_routed = 0
    for i in range(5):
        request = Request(
            tenant_id="tenant_001",
            pillar="test_pillar",
            agent_type="test_agent",
            aml_level=3,
            priority=RequestPriority.NORMAL,
            payload={"test": i}
        )
        
        worker = await router.route_request(request)
        if worker:
            requests_routed += 1
            # Update load to simulate work
            router.update_worker_metrics(
                worker.id,
                current_load=worker.current_load + 1
            )
    
    assert requests_routed > 0, "No requests were routed"
    print(f"âœ… Routed {requests_routed}/5 requests")
    
    # Test circuit breaker
    print("\nTesting circuit breaker...")
    
    # Simulate high error rate on first worker
    router.update_worker_metrics(
        workers[0].id,
        error_rate=0.6  # 60% error rate
    )
    
    # Check if circuit breaker opens
    is_open = router._worker_pool.check_circuit_breaker(
        workers[0].id,
        route_config
    )
    
    assert is_open, "Circuit breaker should open for 60% error rate"
    print("âœ… Circuit breaker opened for failing worker")
    
    # Test sticky sessions
    print("\nTesting sticky sessions...")
    
    request_with_lineage = Request(
        tenant_id="tenant_001",
        pillar="test_pillar",
        agent_type="test_agent",
        aml_level=3,
        lineage_id="trace_001"
    )
    
    # Route first request
    first_worker = await router.route_request(request_with_lineage)
    
    # Route second request with same lineage
    second_worker = await router.route_request(request_with_lineage)
    
    if first_worker and second_worker:
        assert first_worker.id == second_worker.id, "Sticky session should route to same worker"
        print("âœ… Sticky sessions working correctly")
    
    # Get routing stats
    stats = router.get_routing_stats()
    print(f"\nðŸ“Š Routing Statistics:")
    print(f"   Total workers: {stats['pool_stats']['total_workers']}")
    print(f"   Utilization: {stats['pool_stats']['utilization']:.1%}")
    print(f"   Circuit breakers open: {stats['pool_stats']['circuit_breakers_open']}")
    
    return router


async def test_orchestrator_kernel():
    """Test orchestrator kernel integration."""
    print("\nðŸ§ª Testing Orchestrator Kernel")
    print("-" * 40)
    
    # Create kernel configuration
    kernel_config = KernelConfig(
        enable_observability=False,  # Disable for testing
        enable_policy_checks=True,
        enable_event_bus=False,
        enable_lineage_tracking=False,
        health_check_interval=60,  # Longer interval for testing
        route_config=RouteConfig(
            load_balancing_strategy="least_connections"
        )
    )
    
    kernel = OrchestratorKernel(kernel_config)
    
    # Initialize kernel
    await kernel.initialize()
    
    assert kernel.status.value == "running", f"Kernel not running: {kernel.status}"
    print("âœ… Kernel initialized successfully")
    
    # Register test workers
    for i in range(2):
        worker = WorkerNode(
            name=f"kernel-worker-{i+1}",
            host=f"kernel-worker-{i+1}.local",
            port=8080,
            pillar="test_pillar",
            agent_types={"test_agent"},
            capacity=3
        )
        kernel._request_router.register_worker(worker)
    
    # Register test graph
    test_graph = {
        "name": "test_pillar_test_agent",
        "version": "1.0.0",
        "nodes": [
            {
                "id": "process",
                "name": "Process Request",
                "type": "llm_call",
                "config": {"prompt": "Process test request"}
            }
        ],
        "edges": [],
        "entry_node": "process"
    }
    
    version = await kernel.register_graph(test_graph, format="python")
    assert version == "v1.0.0", f"Unexpected version: {version}"
    print(f"âœ… Registered test graph version {version}")
    
    # Test request handling
    request = Request(
        tenant_id="tenant_001",
        pillar="test_pillar",
        agent_type="test_agent",
        aml_level=3,
        payload={"message": "Test request"},
        lineage_id="kernel_test_001"
    )
    
    try:
        # This will fail without full integration setup, but we can test the flow
        result = await kernel.handle_request(request)
    except Exception as e:
        # Expected in test environment
        print(f"â„¹ï¸ Request handling failed as expected in test: {type(e).__name__}")
    
    # Check kernel status
    status = kernel.get_kernel_status()
    
    assert status["status"] == "running", "Kernel should still be running"
    assert status["registered_graphs"] > 0, "No graphs registered"
    assert status["metrics"]["requests_received"] > 0, "Request not counted"
    
    print("\nðŸ“Š Kernel Status:")
    print(f"   Status: {status['status']}")
    print(f"   Registered graphs: {status['registered_graphs']}")
    print(f"   Requests received: {status['metrics']['requests_received']}")
    print(f"   Active workers: {status['routing_stats']['pool_stats']['total_workers']}")
    
    # Shutdown kernel
    await kernel.shutdown()
    assert kernel.status.value == "stopped", "Kernel not stopped properly"
    print("âœ… Kernel shutdown complete")
    
    return kernel


async def test_load_balancer_strategies():
    """Test different load balancing strategies."""
    print("\nðŸ§ª Testing Load Balancer Strategies")
    print("-" * 40)
    
    # Create test workers with different loads
    workers = []
    for i in range(3):
        worker = WorkerNode(
            name=f"lb-worker-{i+1}",
            host=f"lb-worker-{i+1}.local",
            port=8080,
            pillar="test",
            agent_types={"test"},
            capacity=10,
            current_load=i * 2,  # 0, 2, 4
            cpu_usage=20 + i * 20,  # 20%, 40%, 60%
            latency_ms=50 + i * 50  # 50ms, 100ms, 150ms
        )
        workers.append(worker)
    
    request = Request(
        pillar="test",
        agent_type="test",
        aml_level=3
    )
    
    # Test Weighted Round Robin
    print("\nTesting Weighted Round Robin...")
    wrr_balancer = WeightedRoundRobinLoadBalancer()
    
    selections = {}
    for _ in range(100):
        selected = await wrr_balancer.select_worker(workers, request)
        if selected:
            selections[selected.name] = selections.get(selected.name, 0) + 1
    
    print("Distribution after 100 requests:")
    for name, count in sorted(selections.items()):
        print(f"   {name}: {count} requests")
    
    # Worker with best health score should get most requests
    assert max(selections, key=selections.get) == "lb-worker-1", \
        "Healthiest worker should get most requests"
    print("âœ… Weighted round robin favors healthy workers")
    
    # Test Least Connections
    print("\nTesting Least Connections...")
    from orchestrator_kernel.request_router import LeastConnectionsLoadBalancer
    
    lc_balancer = LeastConnectionsLoadBalancer()
    selected = await lc_balancer.select_worker(workers, request)
    
    assert selected.name == "lb-worker-1", \
        "Should select worker with least connections"
    print("âœ… Least connections selects least loaded worker")
    
    return True


async def main():
    """Run all orchestrator kernel tests."""
    print("ðŸš€ Orchestrator Kernel Integration Tests")
    print("=" * 60)
    print("Testing all components of the Orchestrator Kernel:")
    print("- Execution graph management")
    print("- ReAct loop with policy enforcement")
    print("- Request routing and load balancing")
    print("- Kernel integration and lifecycle")
    print("=" * 60)
    
    try:
        # Run component tests
        await test_execution_graph()
        await test_react_loop()
        await test_request_routing()
        await test_load_balancer_strategies()
        await test_orchestrator_kernel()
        
        print("\n" + "="*60)
        print("ðŸŽ‰ ALL ORCHESTRATOR KERNEL TESTS PASSED!")
        print("="*60)
        print("\nOrchestrator Kernel capabilities verified:")
        print("âœ… Graph compilation and versioned execution")
        print("âœ… ReAct loop with Think-Act-Observe pattern")
        print("âœ… Policy enforcement (AML and OPA hooks)")
        print("âœ… Smart request routing with health awareness")
        print("âœ… Circuit breaker pattern for resilience")
        print("âœ… Sticky sessions for request affinity")
        print("âœ… Multiple load balancing strategies")
        print("âœ… Kernel lifecycle management")
        
        print("\nThe Orchestrator Kernel provides:")
        print("ðŸŽ¯ Unified execution management for all agents")
        print("ðŸ”’ Single policy enforcement point")
        print("ðŸ”„ Operational resilience and failover")
        print("ðŸ“Š Full observability and tracing")
        print("âš¡ Elastic scaling capabilities")
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())